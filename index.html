<!DOCTYPE html>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Goal Conditioned Imitation Learning using Score-based Diffusion Policies</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  
  <style>
    h1 {
      font-size: 46px; 
    }
    h2 {
      text-align: center;
    }
    .paper-info {
      display: flex;
      flex-direction: row;
      justify-content: center;
      align-items: center;
      gap: 20px;
      font-size: 18px; /* Adjust text size of code and conference info */
    }
    .authors {
     

      text-align: center;
      margin-top: 10px;
    }
    
  </style>
  </style>
</head>
<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-top: 50px;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:2.5%;width:100%;vertical-align:middle">
          <h1>Goal Conditioned Imitation Learning using Score-based Diffusion Policies</h1>
          <div class="paper-info">
            <p>RSS 2023</p>
            <a href="https://arxiv.org/pdf/2304.02532">Paper</a>
            <a href="https://your_code_link">Code</a>
          </div>
          <div class="authors">
            <a href="https://irl.anthropomatik.kit.edu/21_65.php">Moritz Reuss</a>,
            <a href="https://irl.anthropomatik.kit.edu/21_67.php">Maximilian Li</a>,
            <a href="https://irl.anthropomatik.kit.edu/21_78.php">Xiaogang Jia</a>,
            <a href="http://rudolf.intuitive-robots.net/">Rudolf Lioutikov</a> <br>
                            
          </div>
        </td>
      </tr>
      <tr>
        <td style="padding:2.5%;width:100%;vertical-align:middle">
          <h2>Abstract</h2>
          <div class="abstract-block"></div>
          <p>
      We propose a new policy representation based on score-based diffusion models (SDMs).
      We apply our new policy representation in the domain of Goal-Conditioned Imitation Learning (GCIL) to learn
      general-purpose goal-specified policies from large uncurated datasets without rewards.
      Our new goal-conditioned policy architecture "<b>BE</b>bhavior generation with <b>S</b>c<b>O</b>re-based Diffusion
      Policies" (BESO) leverages a generative, score-based diffusion model as its policy.
      BESO decouples the learning of the score model from the inference sampling process, and, hence
      allows for fast sampling strategies to generate goal-specified behavior in just 3 denoising steps, compared to 30+ steps
      of other diffusion based policies.
      Furthermore, BESO is highly expressive and can effectively capture multi-modality present in the solution space of the
      play data. Unlike previous methods such as Latent Plans or C-Bet, BESO does not rely on complex hierarchical policies or additional
      clustering for effective goal-conditioned behavior learning. Finally, we show how BESO can even be used to learn a
      goal-independent policy from play-data using classifier-free guidance. To the best of our knowledge this is the first
      work that a) represents a behavior policy based on such a decoupled SDM b) learns an SDM based policy in the domain of
      GCIL and c) provides a way to simultaneously learn a goal-dependent and a goal-independent policy from play-data.
      We evaluate BESO through detailed simulation and show that it consistently outperforms several state-of-the-art
      goal-conditioned imitation learning methods on challenging benchmarks.
      We additionally provide extensive ablation studies and experiments to demonstrate the effectiveness of our method for
      effective goal-conditioned behavior generation.</p>
  </p>
</div>
</td>
</tr>
<tr>
  <td style="padding:2.5%;width:100%;vertical-align:middle">
    <img src="data/beso_paper_vis.png" alt="Description of the image" style="display:block;margin-left:auto;margin-right:auto;width:100%">
  </td>
</tr>
<tr>
  <td style="padding:2.5%;width:100%;vertical-align:middle">
    <h2>Method</h2>
    <div class="abstract-block"></div>
    <p>
      This is an example of a new section of text with a heading. You can replace this with your own content.
    </p>
  </div>
  </td>
</tr>
<tr>
  <td style="padding:2.5%;width:100%;vertical-align:middle">
    <h2>Experiments</h2>
    <div class="abstract-block"></div>
    <p>
    This is an example of a new section of text with a heading. You can replace this with your own content.
    </p>
  </div>
  </td>
</tr>
<tr>
  <td style="padding:2.5%;width:100%;vertical-align:middle">
    <heading>BibTeX</heading>
    <p>To cite this paper:</p>
    <pre>
      <code>
          @inproceedings{reuss2023goal,
            title={Goal Conditioned Imitation Learning using Score-based Diffusion Policies},
            author={Reuss, Moritz and Li, Maximilian and Jia, Xiaogang and Lioutikov, Rudolf},
            booktitle={Robotics: Science and Systems},
            year={2023}
          }
      </code>
    </pre>
  </td>
</tr>
</tbody>
</table>
</body>
</html>




